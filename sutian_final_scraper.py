# -*- coding: utf-8 -*-
"""
æ•™è‚²éƒ¨å°èªè¾­å…¸æœ€çµ‚ç‰ˆçˆ¬èŸ²
Final Version - Manual Operation Style Scraper with Missing Words Report
æ‰‹å‹•æ“ä½œé¢¨æ ¼ + ç¼ºå¤±å–®å­—å ±å‘Š
"""

import requests
import time
import json
import re
import pandas as pd
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from typing import List, Dict, Optional, Tuple
import urllib.parse
from pathlib import Path
import os

class SutianFinalScraper:
    """æœ€çµ‚ç‰ˆæ‰‹å‹•æ“ä½œé¢¨æ ¼çˆ¬èŸ²ï¼ˆå«ç¼ºå¤±å–®å­—å ±å‘Šï¼‰"""
    
    def __init__(self):
        self.session = requests.Session()
        
        try:
            self.ua = UserAgent()
            user_agent = self.ua.random
        except:
            user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        
        self.session.headers.update({
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Referer': 'https://sutian.moe.edu.tw/',
        })
        
        # ç¦ç”¨SSLè­¦å‘Š
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        self.session.verify = False
    
    def search_word_examples(self, word: str) -> List[Dict[str, str]]:
        """æ­¥é©Ÿ1ï¼šè¼¸å…¥å–®å­—ï¼Œç²å–æ‰€æœ‰ç”¨ä¾‹"""
        print(f"ğŸ” è¼¸å…¥å–®å­—ï¼š{word}")
        
        try:
            # æ§‹å»ºæŸ¥è©¢URLï¼ˆå°±åƒåœ¨ç¶²é ä¸Šè¼¸å…¥å–®å­—ï¼‰
            params = {
                'lui': 'tai_ku',  # ç”¨è‡ºç£å°èªæŸ¥ç”¨ä¾‹
                'tsha': word
            }
            
            search_url = f"https://sutian.moe.edu.tw/zh-hant/tshiau/?{urllib.parse.urlencode(params)}"
            print(f"   ğŸ“¡ æŸ¥è©¢ç¶²å€ï¼š{search_url}")
            
            response = self.session.get(search_url, timeout=15)
            
            if response.status_code == 200:
                examples = self._parse_webpage_examples(response.text, word)
                print(f"   âœ… æ‰¾åˆ° {len(examples)} å€‹å¯é¸ç”¨ä¾‹")
                return examples
            else:
                print(f"   âŒ ç¶²é è¼‰å…¥å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"   âŒ æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
    
    def _parse_webpage_examples(self, html: str, word: str) -> List[Dict[str, str]]:
        """è§£æç¶²é ï¼Œæå–æ‰€æœ‰ç”¨ä¾‹ï¼ˆå¦‚åŒç€è¦½ç¶²é ï¼‰"""
        examples = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # æ ¹æ“šå¯¦éš›ç¶²é çµæ§‹ï¼ŒæŸ¥æ‰¾ç·¨è™Ÿçš„ç”¨ä¾‹ï¼ˆ1. 2. 3. 4.ï¼‰
            numbered_headers = soup.find_all('h2')
            
            print(f"   ğŸ“‹ ç¶²é é¡¯ç¤º {len(numbered_headers)} å€‹ç”¨ä¾‹é¸é …")
            
            for i, h2_tag in enumerate(numbered_headers, 1):
                try:
                    example_data = self._extract_single_example(h2_tag, word, i)
                    if example_data:
                        examples.append(example_data)
                        print(f"      {i}. {example_data['taiwanese_sentence'][:30]}...")
                    else:
                        print(f"      {i}. ç„¡æ•ˆç”¨ä¾‹")
                        
                except Exception as e:
                    print(f"      {i}. è§£æéŒ¯èª¤: {e}")
                    continue
            
            return examples
            
        except Exception as e:
            print(f"   âŒ ç¶²é è§£æå¤±æ•—: {e}")
            return []
    
    def _extract_single_example(self, h2_tag, word: str, index: int) -> Optional[Dict[str, str]]:
        """æ“·å–å–®å€‹ç”¨ä¾‹çš„ä¸‰è¦ç´ ï¼šå°èªä¾‹å¥ã€å°ç¾…æ‹¼éŸ³ã€ä¸­æ–‡ç¿»è­¯"""
        try:
            # 1. æ“·å–å°èªä¾‹å¥ï¼ˆh2æ¨™ç±¤æ–‡å­—ï¼Œå»æ‰ç·¨è™Ÿï¼‰
            h2_text = h2_tag.get_text(strip=True)
            taiwanese_sentence = re.sub(r'^\d+\.\s*', '', h2_text).strip()
            
            if not taiwanese_sentence or len(taiwanese_sentence) < 5:
                return None
            
            # 2. æ”¶é›†h2å¾Œçš„å…§å®¹ä¾†æ‰¾å°ç¾…æ‹¼éŸ³å’Œä¸­æ–‡ç¿»è­¯
            content_parts = []
            current = h2_tag
            
            # æ”¶é›†ç›¸é—œçš„å¾ŒçºŒå…§å®¹
            for _ in range(15):  # é™åˆ¶æœå°‹ç¯„åœ
                if current.next_sibling:
                    current = current.next_sibling
                    
                    # é‡åˆ°ä¸‹ä¸€å€‹ç·¨è™Ÿå°±åœæ­¢
                    if (hasattr(current, 'name') and current.name == 'h2' and 
                        re.match(r'^\d+\.\s*', current.get_text(strip=True))):
                        break
                    
                    if hasattr(current, 'get_text'):
                        text = current.get_text(strip=True)
                        if text:
                            content_parts.append(text)
                    elif isinstance(current, str) and current.strip():
                        content_parts.append(current.strip())
                else:
                    break
            
            full_content = '\n'.join(content_parts)
            
            # 3. æ“·å–å°ç¾…æ‹¼éŸ³ï¼ˆå®Œæ•´ç‰ˆï¼‰
            tailo_pronunciation = self._extract_tailo_carefully(full_content)
            
            # 4. æ“·å–ä¸­æ–‡ç¿»è­¯ï¼ˆå¾æ‹¬è™Ÿä¸­ï¼‰
            chinese_translation = self._extract_chinese_carefully(full_content)
            
            # 5. æ“·å–ä¾†æºè©ç›®
            source_word = self._extract_source_carefully(full_content)
            
            # åªæœ‰ä¸‰è¦ç´ éƒ½å­˜åœ¨æ‰ç®—æœ‰æ•ˆç”¨ä¾‹
            if taiwanese_sentence and (tailo_pronunciation or chinese_translation):
                return {
                    'index': index,
                    'word': word,
                    'taiwanese_sentence': taiwanese_sentence,
                    'tailo_pronunciation': tailo_pronunciation,
                    'chinese_translation': chinese_translation,
                    'source_word': source_word or word,
                    'source': 'æ•™è‚²éƒ¨è‡ºç£å°èªå¸¸ç”¨è©è¾­å…¸'
                }
            
            return None
            
        except Exception as e:
            print(f"      æ“·å–å¤±æ•—ï¼š{e}")
            return None
    
    def _extract_tailo_carefully(self, content: str) -> str:
        """ä»”ç´°æ“·å–å°ç¾…æ‹¼éŸ³ï¼ˆé¿å…æˆªæ–·ï¼‰"""
        if not content:
            return ''
        
        # ç§»é™¤å¹²æ“¾æ–‡å­—
        cleaned = re.sub(r'æ’­æ”¾ç”¨ä¾‹[^ã€‚]*', '', content)
        cleaned = re.sub(r'ä¾†æºè©ç›®[^ã€‚]*', '', cleaned)
        
        # å°‹æ‰¾å°ç¾…æ‹¼éŸ³æ¨¡å¼
        lines = cleaned.split('\n')
        
        best_tailo = ''
        max_score = 0
        
        for line in lines:
            line = line.strip()
            if len(line) < 10:
                continue
                
            score = self._score_tailo_line(line)
            if score > max_score:
                best_tailo = line
                max_score = score
        
        if best_tailo:
            best_tailo = self._clean_tailo_carefully(best_tailo)
        
        return best_tailo
    
    def _score_tailo_line(self, line: str) -> float:
        """è©•åˆ†å°ç¾…æ‹¼éŸ³è¡Œçš„å¯èƒ½æ€§"""
        score = 0
        
        tailo_chars = 'Ã¢ÃªÃ®Ã´Ã»ÄÄ“Ä«ÅÅ«ÇÄ›ÇÇ’Ç”Ã Ã¨Ã¬Ã²Ã¹'
        special_count = sum(1 for char in line if char in tailo_chars)
        score += special_count * 3
        
        latin_count = sum(1 for char in line if char.isalpha())
        total_chars = len(re.sub(r'\s+', '', line))
        if total_chars > 0:
            latin_ratio = latin_count / total_chars
            if latin_ratio > 0.6:
                score += 10
        
        chinese_count = sum(1 for char in line if '\u4e00' <= char <= '\u9fff')
        if chinese_count == 0:
            score += 5
        else:
            score -= chinese_count
        
        bad_words = ['æ’­æ”¾', 'æœå°‹', 'è¾­å…¸', 'ä¾†æº', 'http']
        for bad_word in bad_words:
            if bad_word in line:
                score -= 10
        
        return score
    
    def _clean_tailo_carefully(self, tailo: str) -> str:
        """ä»”ç´°æ¸…ç†å°ç¾…æ‹¼éŸ³"""
        tailo = re.sub(r'^æ’­æ”¾ç”¨ä¾‹', '', tailo)
        tailo = re.sub(r'æ’­æ”¾.*$', '', tailo)
        tailo = re.sub(r'ä¾†æº.*$', '', tailo)
        tailo = re.sub(r'^[.,;:!?()\'"]+|[.,;:!?()\'"]+$', '', tailo)
        tailo = re.sub(r'\s+', ' ', tailo).strip()
        return tailo
    
    def _extract_chinese_carefully(self, content: str) -> str:
        """ä»”ç´°æ“·å–ä¸­æ–‡ç¿»è­¯"""
        if not content:
            return ''
        
        bracket_patterns = [
            r'[\(ï¼ˆ]([^)ï¼‰]{5,})[\)ï¼‰]',
        ]
        
        for pattern in bracket_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if (len(match) > 3 and 
                    self._is_valid_chinese(match) and
                    'æ’­æ”¾' not in match and 'æœå°‹' not in match):
                    return match.strip()
        
        return ''
    
    def _is_valid_chinese(self, text: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ä¸­æ–‡æ–‡å­—"""
        if not text:
            return False
        
        chinese_count = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        total_count = len(text)
        
        return chinese_count / total_count > 0.5 if total_count > 0 else False
    
    def _extract_source_carefully(self, content: str) -> str:
        """ä»”ç´°æ“·å–ä¾†æºè©ç›®"""
        source_patterns = [
            r'ä¾†æºè©ç›®[ï¼š:\s]*([^\s\n]+)',
        ]
        
        for pattern in source_patterns:
            match = re.search(pattern, content)
            if match:
                source = match.group(1).strip()
                source = re.sub(r'æ’­æ”¾.*$', '', source)
                return source.strip()
        
        return ''
    
    def select_best_example(self, examples: List[Dict[str, str]]) -> Optional[Dict[str, str]]:
        """æ­¥é©Ÿ2ï¼šå¾ç”¨ä¾‹ä¸­é¸æ“‡æœ€ä½³çš„ä¸€å€‹ï¼ˆæ¨¡æ“¬äººå·¥é¸æ“‡ï¼‰"""
        if not examples:
            return None
        
        if len(examples) == 1:
            print(f"   ğŸ“Œ è‡ªå‹•é¸æ“‡å”¯ä¸€ç”¨ä¾‹")
            return examples[0]
        
        def rate_example(example):
            score = 0
            
            if example.get('tailo_pronunciation'):
                score += 30
                score += len(example['tailo_pronunciation']) * 0.5
            
            if example.get('chinese_translation'):
                score += 20
            
            taiwanese = example.get('taiwanese_sentence', '')
            if taiwanese:
                score += len(taiwanese) * 0.3
                if any(punct in taiwanese for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ']):
                    score += 10
            
            return score
        
        examples.sort(key=rate_example, reverse=True)
        best = examples[0]
        
        print(f"   ğŸ“Œ é¸æ“‡æœ€ä½³ç”¨ä¾‹ï¼š{best['taiwanese_sentence'][:30]}...")
        return best
    
    def save_extracted_data(self, word: str, example: Dict[str, str]) -> Dict[str, str]:
        """æ­¥é©Ÿ4ï¼šå„²å­˜ç®¡ç†æ“·å–çš„ä¸‰è¦ç´ """
        if not example:
            return None
        
        record = {
            'word': word,
            'taiwanese_sentence': example.get('taiwanese_sentence', ''),
            'tailo_pronunciation': example.get('tailo_pronunciation', ''),
            'chinese_translation': example.get('chinese_translation', ''),
            'source_word': example.get('source_word', ''),
            'extraction_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'source': example.get('source', ''),
            'data_quality': self._assess_data_quality(example)
        }
        
        return record
    
    def _assess_data_quality(self, example: Dict[str, str]) -> str:
        """è©•ä¼°è³‡æ–™å“è³ª"""
        has_tailo = bool(example.get('tailo_pronunciation'))
        has_chinese = bool(example.get('chinese_translation'))
        has_taiwanese = bool(example.get('taiwanese_sentence'))
        
        if has_tailo and has_chinese and has_taiwanese:
            return 'å®Œæ•´'
        elif (has_tailo and has_taiwanese) or (has_chinese and has_taiwanese):
            return 'è‰¯å¥½'
        elif has_taiwanese:
            return 'åŸºæœ¬'
        else:
            return 'ä¸å®Œæ•´'
    
    def process_word_manual_style(self, word: str) -> Tuple[Optional[Dict[str, str]], str]:
        """å®Œæ•´æ¨¡æ“¬æ‰‹å‹•æ“ä½œæµç¨‹ï¼Œè¿”å›çµæœå’Œç‹€æ…‹"""
        print(f"\nğŸ¯ æ‰‹å‹•æ“ä½œæµç¨‹ï¼š{word}")
        print("-" * 50)
        
        # æ­¥é©Ÿ1ï¼šè¼¸å…¥å–®å­—ï¼Œç²å–ç”¨ä¾‹
        examples = self.search_word_examples(word)
        if not examples:
            print("   âŒ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„ç”¨ä¾‹")
            return None, "ç„¡ç”¨ä¾‹"
        
        # æ­¥é©Ÿ2ï¼šé¸æ“‡æœ€ä½³ç”¨ä¾‹
        selected = self.select_best_example(examples)
        if not selected:
            print("   âŒ ç„¡æ³•é¸æ“‡æœ‰æ•ˆç”¨ä¾‹")
            return None, "ç„¡æ•ˆç”¨ä¾‹"
        
        # æ­¥é©Ÿ3ï¼šæ“·å–ä¸‰è¦ç´ ä¸¦é¡¯ç¤º
        print(f"   ğŸ“ æ“·å–çµæœï¼š")
        print(f"      å°èªï¼š{selected.get('taiwanese_sentence', 'ç„¡')}")
        print(f"      å°ç¾…ï¼š{selected.get('tailo_pronunciation', 'ç„¡')}")
        print(f"      ä¸­æ–‡ï¼š{selected.get('chinese_translation', 'ç„¡')}")
        
        # æ­¥é©Ÿ4ï¼šå„²å­˜ç®¡ç†
        record = self.save_extracted_data(word, selected)
        print(f"   ğŸ’¾ è³‡æ–™å“è³ªï¼š{record.get('data_quality', 'æœªçŸ¥')}")
        
        return record, "æˆåŠŸ"
    
    def process_wordlist_with_missing_report(self, wordlist: List[str]) -> Tuple[List[Dict[str, str]], List[str]]:
        """æ‰¹æ¬¡è™•ç†å–®å­—åˆ—è¡¨ä¸¦ç”¢ç”Ÿç¼ºå¤±å ±å‘Š"""
        print(f"\nğŸ“š æ‰¹æ¬¡æ‰‹å‹•æ“ä½œæ¨¡å¼ï¼ˆå«ç¼ºå¤±å ±å‘Šï¼‰")
        print(f"ğŸ¯ è™•ç† {len(wordlist)} å€‹å–®å­—")
        print("=" * 60)
        
        successful_results = []
        missing_words = []
        
        for i, word in enumerate(wordlist, 1):
            print(f"\né€²åº¦ {i:2d}/{len(wordlist)}")
            
            try:
                record, status = self.process_word_manual_style(word)
                if record:
                    successful_results.append(record)
                    print(f"   âœ… æˆåŠŸæ“·å–")
                else:
                    missing_words.append({
                        'word': word,
                        'reason': status,
                        'index': i
                    })
                    print(f"   âŒ æ“·å–å¤±æ•—ï¼š{status}")
                
                # æ¨¡æ“¬äººå·¥æ“ä½œé–“éš”
                if i < len(wordlist):
                    time.sleep(2.5)
                    
            except Exception as e:
                missing_words.append({
                    'word': word,
                    'reason': f"éŒ¯èª¤: {e}",
                    'index': i
                })
                print(f"   âŒ è™•ç†éŒ¯èª¤: {e}")
                continue
        
        return successful_results, missing_words
    
    def save_results_with_missing_report(self, results: List[Dict], missing_words: List[Dict], title: str = "æœ€çµ‚çµæœ") -> Dict[str, str]:
        """å„²å­˜çµæœä¸¦åŒ…å«ç¼ºå¤±å–®å­—å ±å‘Š"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)
        
        # å»ºç«‹è¼¸å‡ºç›®éŒ„
        output_dir = f"final_{safe_title}"
        os.makedirs(output_dir, exist_ok=True)
        
        total_words = len(results) + len(missing_words)
        
        # å„²å­˜ä¸»è¦çµæœJSON
        json_file = f"{output_dir}/{safe_title}_final_{timestamp}.json"
        json_data = {
            'metadata': {
                'scraper_type': 'æœ€çµ‚ç‰ˆæ‰‹å‹•æ“ä½œé¢¨æ ¼çˆ¬èŸ²',
                'operation_flow': 'è¼¸å…¥å–®å­— â†’ é¸æ“‡ç”¨ä¾‹ â†’ æ“·å–ä¸‰è¦ç´  â†’ å„²å­˜ç®¡ç†',
                'source_url': 'https://sutian.moe.edu.tw/zh-hant/tshiau/',
                'extraction_date': timestamp,
                'statistics': {
                    'total_words': total_words,
                    'successful_extractions': len(results),
                    'missing_words': len(missing_words),
                    'success_rate': f"{len(results)/total_words*100:.1f}%" if total_words > 0 else "0%",
                    'quality_stats': self._calculate_quality_stats(results)
                }
            },
            'successful_records': results,
            'missing_words': missing_words
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        # å„²å­˜æˆåŠŸè¨˜éŒ„CSV
        csv_file = f"{output_dir}/{safe_title}_successful_{timestamp}.csv"
        if results:
            csv_data = []
            
            for record in results:
                csv_data.append({
                    'å–®å­—': record['word'],
                    'å°èªä¾‹å¥': record['taiwanese_sentence'],
                    'å°ç¾…æ‹¼éŸ³': record['tailo_pronunciation'],
                    'ä¸­æ–‡ç¿»è­¯': record['chinese_translation'],
                    'ä¾†æºè©ç›®': record['source_word'],
                    'è³‡æ–™å“è³ª': record['data_quality'],
                    'æ“·å–æ™‚é–“': record['extraction_time'],
                    'è³‡æ–™ä¾†æº': record['source']
                })
            
            df = pd.DataFrame(csv_data)
            quality_order = {'å®Œæ•´': 4, 'è‰¯å¥½': 3, 'åŸºæœ¬': 2, 'ä¸å®Œæ•´': 1}
            df['å“è³ªåˆ†æ•¸'] = df['è³‡æ–™å“è³ª'].map(quality_order).fillna(0)
            df = df.sort_values(['å“è³ªåˆ†æ•¸', 'å–®å­—'], ascending=[False, True])
            df = df.drop('å“è³ªåˆ†æ•¸', axis=1)
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        # å„²å­˜ç¼ºå¤±å–®å­—CSV
        missing_csv_file = f"{output_dir}/{safe_title}_missing_words_{timestamp}.csv"
        if missing_words:
            missing_df = pd.DataFrame(missing_words)
            missing_df = missing_df.sort_values('index')
            missing_df.to_csv(missing_csv_file, index=False, encoding='utf-8-sig')
        
        # å„²å­˜å®Œæ•´å ±å‘ŠTXT
        txt_file = f"{output_dir}/{safe_title}_complete_report_{timestamp}.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(f"æœ€çµ‚ç‰ˆå°èªè¾­å…¸æ“·å–å®Œæ•´å ±å‘Š - {title}\n")
            f.write("=" * 80 + "\n")
            f.write(f"ğŸ¯ æ“ä½œæµç¨‹ï¼šè¼¸å…¥å–®å­— â†’ é¸æ“‡ç”¨ä¾‹ â†’ æ“·å–ä¸‰è¦ç´  â†’ å„²å­˜ç®¡ç†\n")
            f.write(f"ğŸ“Š å®Œæ•´çµ±è¨ˆï¼š\n")
            f.write(f"   - ç¸½è¨ˆå–®å­—ï¼š{total_words} å€‹\n")
            f.write(f"   - æˆåŠŸæ“·å–ï¼š{len(results)} å€‹\n")
            f.write(f"   - ç¼ºå¤±å–®å­—ï¼š{len(missing_words)} å€‹\n")
            f.write(f"   - æˆåŠŸç‡ï¼š{len(results)/total_words*100:.1f}%\n" if total_words > 0 else "   - æˆåŠŸç‡ï¼š0%\n")
            f.write(f"â° æ“·å–æ™‚é–“ï¼š{timestamp}\n\n")
            
            # æˆåŠŸæ“·å–çš„çµæœ
            if results:
                complete_records = [r for r in results if r['data_quality'] == 'å®Œæ•´']
                good_records = [r for r in results if r['data_quality'] == 'è‰¯å¥½']
                basic_records = [r for r in results if r['data_quality'] == 'åŸºæœ¬']
                
                if complete_records:
                    f.write("ğŸŒŸ å®Œæ•´æ“·å–ï¼ˆå°èª+å°ç¾…+ä¸­æ–‡ï¼‰:\n")
                    f.write("-" * 60 + "\n")
                    for i, record in enumerate(complete_records, 1):
                        f.write(f"{i:2d}. {record['word']}\n")
                        f.write(f"    å°èªï¼š{record['taiwanese_sentence']}\n")
                        f.write(f"    å°ç¾…ï¼š{record['tailo_pronunciation']}\n")
                        f.write(f"    ä¸­æ–‡ï¼š{record['chinese_translation']}\n")
                        if record['source_word'] != record['word']:
                            f.write(f"    ä¾†æºï¼š{record['source_word']}\n")
                        f.write("\n")
                
                if good_records:
                    f.write("ğŸ“ è‰¯å¥½æ“·å–:\n")
                    f.write("-" * 60 + "\n")
                    for i, record in enumerate(good_records, 1):
                        f.write(f"{i:2d}. {record['word']}\n")
                        f.write(f"    å°èªï¼š{record['taiwanese_sentence']}\n")
                        if record['tailo_pronunciation']:
                            f.write(f"    å°ç¾…ï¼š{record['tailo_pronunciation']}\n")
                        if record['chinese_translation']:
                            f.write(f"    ä¸­æ–‡ï¼š{record['chinese_translation']}\n")
                        f.write("\n")
                
                if basic_records:
                    f.write("ğŸ“„ åŸºæœ¬æ“·å–:\n")
                    f.write("-" * 60 + "\n")
                    for i, record in enumerate(basic_records, 1):
                        f.write(f"{i:2d}. {record['word']}\n")
                        f.write(f"    å°èªï¼š{record['taiwanese_sentence']}\n")
                        f.write("\n")
            
            # ç¼ºå¤±å–®å­—å ±å‘Š
            if missing_words:
                f.write("âŒ ç¼ºå¤±å–®å­—å ±å‘Š:\n")
                f.write("=" * 60 + "\n")
                f.write(f"ä»¥ä¸‹ {len(missing_words)} å€‹å–®å­—æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„ä¾‹å¥ï¼š\n\n")
                
                # æŒ‰åŸå› åˆ†çµ„
                reasons = {}
                for missing in missing_words:
                    reason = missing['reason']
                    if reason not in reasons:
                        reasons[reason] = []
                    reasons[reason].append(missing['word'])
                
                for reason, words in reasons.items():
                    f.write(f"ğŸ“‹ {reason} ({len(words)}å€‹):\n")
                    for i, word in enumerate(words, 1):
                        if i % 10 == 1:
                            f.write("   ")
                        f.write(f"{word:<8}")
                        if i % 10 == 0:
                            f.write("\n")
                    if len(words) % 10 != 0:
                        f.write("\n")
                    f.write("\n")
                
                f.write("ğŸ’¡ å»ºè­°ï¼š\n")
                f.write("   1. é€™äº›å–®å­—å¯èƒ½åœ¨æ•™è‚²éƒ¨è¾­å…¸ä¸­æ²’æœ‰ç”¨ä¾‹\n")
                f.write("   2. å¯ä»¥å˜—è©¦å…¶ä»–å°èªè¾­å…¸æˆ–è³‡æº\n")
                f.write("   3. æˆ–è€…æ‰‹å‹•æŸ¥è©¢ç›¸é—œçš„åŒç¾©è©\n")
        
        print(f"\nğŸ’¾ æœ€çµ‚çµæœå·²å„²å­˜:")
        print(f"   ğŸ“Š å®Œæ•´JSON: {json_file}")
        if results:
            print(f"   ğŸ“‹ æˆåŠŸCSV: {csv_file}")
        if missing_words:
            print(f"   âŒ ç¼ºå¤±CSV: {missing_csv_file}")
        print(f"   ğŸ“– å®Œæ•´å ±å‘Š: {txt_file}")
        
        # é¡¯ç¤ºç¼ºå¤±æ‘˜è¦
        if missing_words:
            print(f"\nâŒ ç¼ºå¤±å–®å­—æ‘˜è¦ï¼š")
            reasons = {}
            for missing in missing_words:
                reason = missing['reason']
                if reason not in reasons:
                    reasons[reason] = []
                reasons[reason].append(missing['word'])
            
            for reason, words in reasons.items():
                print(f"   {reason}: {len(words)}å€‹")
                sample_words = words[:5]
                print(f"     å¦‚ï¼š{', '.join(sample_words)}{'...' if len(words) > 5 else ''}")
        
        return {
            'json_file': json_file,
            'csv_file': csv_file if results else None,
            'missing_csv_file': missing_csv_file if missing_words else None,
            'txt_file': txt_file,
            'output_dir': output_dir
        }
    
    def _calculate_quality_stats(self, results: List[Dict]) -> Dict[str, int]:
        """è¨ˆç®—å“è³ªçµ±è¨ˆ"""
        stats = {'å®Œæ•´': 0, 'è‰¯å¥½': 0, 'åŸºæœ¬': 0, 'ä¸å®Œæ•´': 0}
        
        for record in results:
            quality = record.get('data_quality', 'ä¸å®Œæ•´')
            if quality in stats:
                stats[quality] += 1
        
        return stats
    
    def cleanup(self):
        """æ¸…ç†è³‡æº"""
        if hasattr(self, 'session'):
            self.session.close()

def main():
    """æœ€çµ‚ç‰ˆçˆ¬èŸ²ä¸»ç¨‹å¼"""
    scraper = SutianFinalScraper()
    
    try:
        print("ğŸ† æ•™è‚²éƒ¨å°èªè¾­å…¸æœ€çµ‚ç‰ˆçˆ¬èŸ²")
        print("=" * 60)
        print("ğŸ“‹ ç‰¹è‰²ï¼šæ‰‹å‹•æ“ä½œé¢¨æ ¼ + ç¼ºå¤±å–®å­—å ±å‘Š")
        print("ğŸ¯ åŠŸèƒ½ï¼šå®Œæ•´è¿½è¹¤å“ªäº›å–®å­—æ²’æœ‰æ‰¾åˆ°ä¾‹å¥")
        print("ğŸ›ï¸ è³‡æ–™ä¾†æºï¼šæ•™è‚²éƒ¨è‡ºç£å°èªå¸¸ç”¨è©è¾­å…¸\n")
        
        while True:
            print("è«‹é¸æ“‡æ“ä½œæ¨¡å¼ï¼š")
            print("1. å–®å­—æ¸¬è©¦ï¼ˆæ‰‹å‹•è¼¸å…¥ï¼‰")
            print("2. å·¥ä½œè¡¨æ‰¹æ¬¡è™•ç†ï¼ˆå«ç¼ºå¤±å ±å‘Šï¼‰")
            print("3. è‡ªå®šç¾©å–®å­—åˆ—è¡¨ï¼ˆå«ç¼ºå¤±å ±å‘Šï¼‰")
            print("0. é€€å‡º")
            
            choice = input("\nè«‹è¼¸å…¥é¸é … (0-3): ").strip()
            
            if choice == '0':
                print("ğŸ‘‹ æ„Ÿè¬ä½¿ç”¨æœ€çµ‚ç‰ˆçˆ¬èŸ²ï¼")
                break
                
            elif choice == '1':
                # å–®å­—æ¸¬è©¦
                word = input("è«‹è¼¸å…¥è¦æ¸¬è©¦çš„å°èªå–®å­—ï¼š").strip()
                if word:
                    result, status = scraper.process_word_manual_style(word)
                    if result:
                        print(f"\nâœ… æˆåŠŸæ“·å–ã€Œ{word}ã€çš„è³‡æ–™")
                    else:
                        print(f"\nâŒ ç„¡æ³•æ“·å–ã€Œ{word}ã€çš„è³‡æ–™ï¼ŒåŸå› ï¼š{status}")
                else:
                    print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„å–®å­—")
                    
            elif choice == '2':
                # å·¥ä½œè¡¨è™•ç†
                excel_file = "è‡ºèªè©å½™0720.xlsx"
                try:
                    excel_obj = pd.ExcelFile(excel_file)
                    worksheets = [name for name in excel_obj.sheet_names if name != "å·¥ä½œè¡¨åˆ†é¡æ¸…å–®"]
                    
                    print(f"\nğŸ“š å¯ç”¨å·¥ä½œè¡¨ï¼š")
                    for i, ws in enumerate(worksheets, 1):
                        print(f"  {i:2d}. {ws}")
                    
                    ws_choice = input(f"\nè«‹é¸æ“‡å·¥ä½œè¡¨ç·¨è™Ÿ (1-{len(worksheets)}): ").strip()
                    ws_index = int(ws_choice) - 1
                    
                    if 0 <= ws_index < len(worksheets):
                        selected_ws = worksheets[ws_index]
                        
                        # æå–å–®å­—
                        df = pd.read_excel(excel_file, sheet_name=selected_ws)
                        words = []
                        
                        for column in df.columns:
                            column_words = df[column].astype(str).tolist()
                            for word in column_words:
                                word = word.strip()
                                if (word and word != 'nan' and 
                                    not word.isdigit() and 
                                    len(word) > 1 and
                                    any('\u4e00' <= char <= '\u9fff' for char in word)):
                                    
                                    cleaned_word = re.sub(r'^\d+\.?\s*', '', word)
                                    cleaned_word = re.sub(r'\([^)]*\)', '', cleaned_word)
                                    cleaned_word = cleaned_word.strip()
                                    
                                    if len(cleaned_word) > 1 and cleaned_word not in ['å…¶ä»–', 'å‚™è¨»', 'èªªæ˜', 'é¡åˆ¥']:
                                        words.append(cleaned_word)
                        
                        # å»é‡
                        words = list(set(words))
                        print(f"\nğŸ“ æ‰¾åˆ° {len(words)} å€‹å–®å­—")
                        
                        # è©¢å•æ˜¯å¦ç¹¼çºŒ
                        confirm = input(f"æ˜¯å¦é–‹å§‹è™•ç†ï¼Ÿ(y/n): ").lower().strip()
                        if confirm == 'y':
                            results, missing_words = scraper.process_wordlist_with_missing_report(words)
                            
                            saved = scraper.save_results_with_missing_report(results, missing_words, selected_ws)
                            if saved:
                                print(f"\nğŸ‰ å·¥ä½œè¡¨ã€Œ{selected_ws}ã€è™•ç†å®Œæˆï¼")
                                print(f"ğŸ“ çµæœå„²å­˜åœ¨ï¼š{saved['output_dir']}")
                        else:
                            print("ğŸ‘‹ å·²å–æ¶ˆæ“ä½œ")
                    else:
                        print("âŒ ç„¡æ•ˆçš„å·¥ä½œè¡¨é¸æ“‡")
                        
                except Exception as e:
                    print(f"âŒ è™•ç†å·¥ä½œè¡¨å¤±æ•—: {e}")
                    
            elif choice == '3':
                # è‡ªå®šç¾©åˆ—è¡¨
                print("è«‹è¼¸å…¥å°èªå–®å­—ï¼ˆä¸€è¡Œä¸€å€‹ï¼Œè¼¸å…¥ç©ºè¡ŒçµæŸï¼‰ï¼š")
                words = []
                while True:
                    word = input().strip()
                    if not word:
                        break
                    words.append(word)
                
                if words:
                    results, missing_words = scraper.process_wordlist_with_missing_report(words)
                    saved = scraper.save_results_with_missing_report(results, missing_words, "è‡ªå®šç¾©åˆ—è¡¨")
                    if saved:
                        print(f"\nğŸ‰ è‡ªå®šç¾©åˆ—è¡¨è™•ç†å®Œæˆï¼")
                        print(f"ğŸ“ çµæœå„²å­˜åœ¨ï¼š{saved['output_dir']}")
                else:
                    print("âŒ æ²’æœ‰è¼¸å…¥æœ‰æ•ˆçš„å–®å­—")
                    
            else:
                print("âŒ ç„¡æ•ˆé¸é …")
    
    finally:
        scraper.cleanup()

if __name__ == "__main__":
    main()

